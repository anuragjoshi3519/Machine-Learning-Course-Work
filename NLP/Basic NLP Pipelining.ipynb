{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NLP Pipelining\n",
    "- *Data Collection*\n",
    "- *Tokenization, stopword,stemming*\n",
    "- *Building a common vocab*\n",
    "- *Vectorizing the documents*\n",
    "- *Performing Classification/Clustering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Data Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'General', 'Assembly', ',', 'which', 'adjourns', 'today', ',', 'has', 'performed', 'in', 'an', 'atmosphere', 'of', 'crisis', 'and', 'struggle', 'from', 'the', 'day', 'it', 'convened', '.']\n"
     ]
    }
   ],
   "source": [
    "data = brown.sents(categories='editorial')\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''It was a rainy day, I was not stasnding alone on the balcony of my house in switzerland.A flock of birds was flying in the sky.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents= sent_tokenize(text)\n",
    "words= word_tokenize(sents[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'a', 'rainy', 'day.day', ',', 'i', 'was', 'not', 'standing', 'alone', 'on', 'the', 'balcony', 'of', 'my', 'house', 'in', 'switzerland.a', 'flock', 'of', 'birds', 'was', 'flying', 'in', 'the', 'sky', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regexTokenizer is used to tokenize sentences with regular expression of our choice\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'a', 'rainy', 'day', 'i', 'was', 'not', 'standing', 'alone', 'on', 'the', 'balcony', 'of', 'my', 'house', 'in', 'switzerland', 'a', 'flock', 'of', 'birds', 'was', 'flying', 'in', 'the', 'sky']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('[a-zA-Z]+')\n",
    "\n",
    "words= tokenizer.tokenize(text.lower())\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Removing Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords=list(stopwords.words('english'))\n",
    "#print(len(stopWords))\n",
    "\n",
    "def removeStopWords(sent):\n",
    "    extract = [w for w in sent if w not in stopWords ]\n",
    "    return extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_words=removeStopWords(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rainy', 'day', 'standing', 'alone', 'balcony', 'house', 'switzerland', 'flock', 'birds', 'flying', 'sky']\n"
     ]
    }
   ],
   "source": [
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Stemming**\n",
    "    1. Snowball Stemmer (Multilingual)\n",
    "    2. Porter Stemmer\n",
    "    3. Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lovely', 'day', 'man', 'came', 'jumped', 'pile', 'garbage', 'seemed', 'going', 'way', 'fast', 'somewhere']\n"
     ]
    }
   ],
   "source": [
    "text1='''It was a lovely day , a man came and jumped over a pile of garbage he seemed to be going a way fast somewhere'''\n",
    "words=tokenizer.tokenize(text1.lower())\n",
    "words=removeStopWords(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soundli\n",
      "teen\n",
      "teenag\n",
      "\n",
      "have\n",
      "hav\n",
      "have\n",
      "\n",
      "awesom\n",
      "awesom\n",
      "awesom\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PorterStemmer,SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "ls=LancasterStemmer()\n",
    "ss=SnowballStemmer('english')\n",
    "\n",
    "print(ps.stem('soundly'))\n",
    "print(ls.stem('teenager'))\n",
    "print(ss.stem('teenager'))\n",
    "print()\n",
    "print(ps.stem('having'))\n",
    "print(ls.stem('having'))\n",
    "print(ss.stem('having'))\n",
    "print()\n",
    "print(ps.stem('awesome'))\n",
    "print(ls.stem('awesome'))\n",
    "print(ss.stem('awesome'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'day', 'man', 'came', 'jump', 'pile', 'garbag', 'seem', 'go', 'way', 'fast', 'somewher']\n"
     ]
    }
   ],
   "source": [
    "words=[ps.stem(w) for w in words ]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to perform all these steps when given a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def prepare(text):\n",
    "    # Tokenize\n",
    "    tokenizer=RegexpTokenizer('[a-zA-Z]+')\n",
    "    wordsList=tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    #Remove Stopword\n",
    "    stopWords=list(stopwords.words('english'))\n",
    "    wordsList = [w for w in wordsList if w not in stopWords ]\n",
    "    \n",
    "      #Stemming & Lemmatization\n",
    "#     ps=PorterStemmer()\n",
    "#     wordsList= [ps.stem(w) for w in wordsList ]\n",
    "#     lm=WordNetLemmatizer()    \n",
    "#     wordsList= [lm.lemmatize(w,pos='v') for w in wordsList ]\n",
    "#     wordsList= [lm.lemmatize(w,pos='a') for w in wordsList ]\n",
    "#     wordsList= [lm.lemmatize(w,pos='n') for w in wordsList ]\n",
    "#     wordsList= [lm.lemmatize(w,pos='s') for w in wordsList ]\n",
    "#     wordsList= [lm.lemmatize(w,pos='r') for w in wordsList ]\n",
    "    \n",
    "    return wordsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lovely', 'day', 'man', 'came', 'jumped', 'pile', 'garbage', 'seemed', 'going', 'way', 'fastest', 'somewhere']\n"
     ]
    }
   ],
   "source": [
    "text1='''It was a lovely day, a man   came and jumped   over\n",
    "a pile of garbage he seemed to be going a way fastest somewhere.'''\n",
    "print(prepare(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building common vocabolary and vectorizing document (Based upon bag of words model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "corpus=[\n",
    "    'It was raining raining raining raining on the semifinals of the cricket world cup held in india,so the match was postponed.',\n",
    "    'India has many mesmerizing historical monuments that every one should visit',\n",
    "    'The teacher that was teaching in south asian university was from india unlike other teachers that were native.',\n",
    "    'News editing team tried there level best to show india as a developing country with fast growing GDP.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(tokenizer=prepare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 4 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0]\n",
      " [0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus=cv.fit_transform(corpus)\n",
    "#print(type(vectorized_corpus))\n",
    "#print(vectorized_corpus)  # sparse matrix\n",
    "vc=vectorized_corpus.toarray()   # dense matrix\n",
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asian', 'best', 'country', 'cricket', 'cup', 'developing', 'editing', 'every', 'fast', 'gdp', 'growing', 'held', 'historical', 'india', 'level', 'many', 'match', 'mesmerizing', 'monuments', 'native', 'news', 'one', 'postponed', 'raining', 'semifinals', 'show', 'south', 'teacher', 'teachers', 'teaching', 'team', 'tried', 'university', 'unlike', 'visit', 'world']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raining': 23, 'semifinals': 24, 'cricket': 3, 'world': 35, 'cup': 4, 'held': 11, 'india': 13, 'match': 16, 'postponed': 22, 'many': 15, 'mesmerizing': 17, 'historical': 12, 'monuments': 18, 'every': 7, 'one': 21, 'visit': 34, 'teacher': 27, 'teaching': 29, 'south': 26, 'asian': 0, 'university': 32, 'unlike': 33, 'teachers': 28, 'native': 19, 'news': 20, 'editing': 6, 'team': 30, 'tried': 31, 'level': 14, 'best': 1, 'show': 25, 'developing': 5, 'country': 2, 'fast': 8, 'growing': 10, 'gdp': 9}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newSentence='This is completely different sentence \"cricket\".'\n",
    "cv.transform([newSentence]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['asian', 'best', 'country', 'developing', 'editing', 'every',\n",
      "       'fast', 'gdp', 'growing', 'mesmerizing', 'monuments', 'native',\n",
      "       'news', 'one'], dtype='<U11')]\n"
     ]
    }
   ],
   "source": [
    "## given a vector , getting the sentence\n",
    "vect=np.ones((22,))\n",
    "vect[3:5]=0\n",
    "vect[11:17]=0\n",
    "print(cv.inverse_transform(vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['cricket', 'cup', 'held', 'india', 'match', 'postponed', 'raining',\n",
       "        'semifinals', 'world'], dtype='<U11')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(vc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is Unigram Bag of Words model: in this each unique word goes into the bag and used as a feature\n",
    "## In Bigram Bag of words model pair of adjacent words together are used as feature , this is done basically to also get a little meaning from sentence (or atleast get the sentiment of the sentence like negative ,positive,etc  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 3 1 1 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0]\n",
      " [0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0]]\n",
      "\n",
      "['asian university', 'best show', 'country fast', 'cricket world', 'cup held', 'developing country', 'editing team', 'every one', 'fast growing', 'growing gdp', 'held india', 'historical monuments', 'india developing', 'india many', 'india match', 'india unlike', 'level best', 'many mesmerizing', 'match postponed', 'mesmerizing historical', 'monuments every', 'news editing', 'one visit', 'raining raining', 'raining semifinals', 'semifinals cricket', 'show india', 'south asian', 'teacher teaching', 'teachers native', 'teaching south', 'team tried', 'tried level', 'university india', 'unlike teachers', 'world cup']\n"
     ]
    }
   ],
   "source": [
    "## Bigrams\n",
    "cv1=CountVectorizer(tokenizer=prepare,ngram_range=(2,2))\n",
    "vc1=cv1.fit_transform(corpus)\n",
    "vc1=vc1.toarray()   # dense matrix\n",
    "print(vc1,end='\\n\\n')\n",
    "print(cv1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
      "  0 0 0 0 0 0 0 0 0 1 4 3 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0\n",
      "  1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0]\n",
      " [0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0\n",
      "  0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0]]\n",
      "\n",
      "['asian', 'asian university', 'best', 'best show', 'country', 'country fast', 'cricket', 'cricket world', 'cup', 'cup held', 'developing', 'developing country', 'editing', 'editing team', 'every', 'every one', 'fast', 'fast growing', 'gdp', 'growing', 'growing gdp', 'held', 'held india', 'historical', 'historical monuments', 'india', 'india developing', 'india many', 'india match', 'india unlike', 'level', 'level best', 'many', 'many mesmerizing', 'match', 'match postponed', 'mesmerizing', 'mesmerizing historical', 'monuments', 'monuments every', 'native', 'news', 'news editing', 'one', 'one visit', 'postponed', 'raining', 'raining raining', 'raining semifinals', 'semifinals', 'semifinals cricket', 'show', 'show india', 'south', 'south asian', 'teacher', 'teacher teaching', 'teachers', 'teachers native', 'teaching', 'teaching south', 'team', 'team tried', 'tried', 'tried level', 'university', 'university india', 'unlike', 'unlike teachers', 'visit', 'world', 'world cup']\n"
     ]
    }
   ],
   "source": [
    "## Bigrams and unigrams both\n",
    "cv1=CountVectorizer(tokenizer=prepare,ngram_range=(1,2))\n",
    "vc1=cv1.fit_transform(corpus)\n",
    "vc1=vc1.toarray()   # dense matrix\n",
    "print(vc1,end='\\n\\n')\n",
    "print(cv1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 2 1 1 1 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0]\n",
      " [0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0]]\n",
      "\n",
      "['asian university india', 'best show india', 'country fast growing', 'cricket world cup', 'cup held india', 'developing country fast', 'editing team tried', 'every one visit', 'fast growing gdp', 'held india match', 'historical monuments every', 'india developing country', 'india many mesmerizing', 'india match postponed', 'india unlike teachers', 'level best show', 'many mesmerizing historical', 'mesmerizing historical monuments', 'monuments every one', 'news editing team', 'raining raining raining', 'raining raining semifinals', 'raining semifinals cricket', 'semifinals cricket world', 'show india developing', 'south asian university', 'teacher teaching south', 'teaching south asian', 'team tried level', 'tried level best', 'university india unlike', 'unlike teachers native', 'world cup held']\n"
     ]
    }
   ],
   "source": [
    "## Trigrams\n",
    "cv1=CountVectorizer(tokenizer=prepare,ngram_range=(3,3))\n",
    "vc1=cv1.fit_transform(corpus)\n",
    "vc1=vc1.toarray()   # dense matrix\n",
    "print(vc1,end='\\n\\n')\n",
    "print(cv1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 3 2 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0\n",
      "  1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0]\n",
      " [0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0\n",
      "  0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0]]\n",
      "\n",
      "['asian university', 'asian university india', 'best show', 'best show india', 'country fast', 'country fast growing', 'cricket world', 'cricket world cup', 'cup held', 'cup held india', 'developing country', 'developing country fast', 'editing team', 'editing team tried', 'every one', 'every one visit', 'fast growing', 'fast growing gdp', 'growing gdp', 'held india', 'held india match', 'historical monuments', 'historical monuments every', 'india developing', 'india developing country', 'india many', 'india many mesmerizing', 'india match', 'india match postponed', 'india unlike', 'india unlike teachers', 'level best', 'level best show', 'many mesmerizing', 'many mesmerizing historical', 'match postponed', 'mesmerizing historical', 'mesmerizing historical monuments', 'monuments every', 'monuments every one', 'news editing', 'news editing team', 'one visit', 'raining raining', 'raining raining raining', 'raining raining semifinals', 'raining semifinals', 'raining semifinals cricket', 'semifinals cricket', 'semifinals cricket world', 'show india', 'show india developing', 'south asian', 'south asian university', 'teacher teaching', 'teacher teaching south', 'teachers native', 'teaching south', 'teaching south asian', 'team tried', 'team tried level', 'tried level', 'tried level best', 'university india', 'university india unlike', 'unlike teachers', 'unlike teachers native', 'world cup', 'world cup held']\n"
     ]
    }
   ],
   "source": [
    "## Trigrams and bigrams\n",
    "cv1=CountVectorizer(tokenizer=prepare,ngram_range=(2,3))\n",
    "vc1=cv1.fit_transform(corpus)\n",
    "vc1=vc1.toarray()   # dense matrix\n",
    "print(vc1,end='\\n\\n')\n",
    "print(cv1.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Normalisation\n",
    "**TF (Term frequency)** of a term is the number of time that term appears in the document.\n",
    "\n",
    "Formula for calculating **Idf (term,corpus) = log ( (n+1)/ (1 + count(D,t) )) + 1**\n",
    "\n",
    "Where,\n",
    "\n",
    "count(D,t) = number of documents containing the term t\n",
    "\n",
    "n= number of documents in corpus\n",
    "\n",
    "**Weight** (for each feature) = **Tf * IDf** \n",
    "\n",
    "**The resulting tf-idf vectors are then normalized by the Euclidean norm (when norm='l2'):**\n",
    "\n",
    "$ v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}} $\n",
    "\n",
    "- It is used for avoiding features that occur very often , because they contain less information, like when in a corpus there are mant different documents having info about different topics and almost each of the document contains words like books, water etc. So these words should have very less weight in performing any ML algo like clustering and classification on the document.\n",
    "- Information aparted by a particular word decreases as the occurences of the word increases across the document.\n",
    "- So we use TF-IDF(Term Frequency - Inverse Document Frequency) normalisation to associate weight to every feature or term in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as', 'asian', 'best', 'country', 'cricket', 'cup', 'developing', 'editing', 'every', 'fast', 'from', 'gdp', 'growing', 'has', 'held', 'historical', 'in', 'india', 'it', 'level', 'many', 'match', 'mesmerizing', 'monuments', 'native', 'news', 'of', 'on', 'one', 'other', 'postponed', 'raining', 'semifinals', 'should', 'show', 'so', 'south', 'teacher', 'teachers', 'teaching', 'team', 'that', 'the', 'there', 'to', 'tried', 'university', 'unlike', 'visit', 'was', 'were', 'with', 'world']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(ngram_range=(1,1))\n",
    "tv=tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 18, 'was': 49, 'raining': 31, 'on': 27, 'the': 42, 'semifinals': 32, 'of': 26, 'cricket': 4, 'world': 52, 'cup': 5, 'held': 14, 'in': 16, 'india': 17, 'so': 35, 'match': 21, 'postponed': 30, 'has': 13, 'many': 20, 'mesmerizing': 22, 'historical': 15, 'monuments': 23, 'that': 41, 'every': 8, 'one': 28, 'should': 33, 'visit': 48, 'teacher': 37, 'teaching': 39, 'south': 36, 'asian': 1, 'university': 46, 'from': 10, 'unlike': 47, 'other': 29, 'teachers': 38, 'were': 50, 'native': 24, 'news': 25, 'editing': 7, 'team': 40, 'tried': 45, 'there': 43, 'level': 19, 'best': 2, 'to': 44, 'show': 34, 'as': 0, 'developing': 6, 'country': 3, 'with': 51, 'fast': 9, 'growing': 12, 'gdp': 11}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['asian', 'from', 'in', 'india', 'native', 'other', 'south',\n",
      "       'teacher', 'teachers', 'teaching', 'that', 'the', 'university',\n",
      "       'unlike', 'was', 'were'], dtype='<U11')]\n",
      "0.23912604584443742\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.inverse_transform(tv[2]))\n",
    "print(tv[2][38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.16672546 0.16672546\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16672546 0.         0.13144827 0.08700426\n",
      "  0.16672546 0.         0.         0.16672546 0.         0.\n",
      "  0.         0.         0.16672546 0.16672546 0.         0.\n",
      "  0.16672546 0.66690183 0.16672546 0.         0.         0.16672546\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.3943448  0.         0.         0.         0.         0.\n",
      "  0.         0.26289653 0.         0.         0.16672546]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.31791864 0.         0.         0.\n",
      "  0.         0.31791864 0.         0.31791864 0.         0.16590314\n",
      "  0.         0.         0.31791864 0.         0.31791864 0.31791864\n",
      "  0.         0.         0.         0.         0.31791864 0.\n",
      "  0.         0.         0.         0.31791864 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25065071\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.31791864 0.         0.         0.         0.        ]\n",
      " [0.         0.23912605 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.23912605 0.\n",
      "  0.         0.         0.         0.         0.18852972 0.12478589\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.23912605 0.         0.         0.         0.         0.23912605\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.23912605 0.23912605 0.23912605 0.23912605 0.         0.37705944\n",
      "  0.18852972 0.         0.         0.         0.23912605 0.23912605\n",
      "  0.         0.37705944 0.23912605 0.         0.        ]\n",
      " [0.24789929 0.         0.24789929 0.24789929 0.         0.\n",
      "  0.24789929 0.24789929 0.         0.24789929 0.         0.24789929\n",
      "  0.24789929 0.         0.         0.         0.         0.12936413\n",
      "  0.         0.24789929 0.         0.         0.         0.\n",
      "  0.         0.24789929 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.24789929 0.\n",
      "  0.         0.         0.         0.         0.24789929 0.\n",
      "  0.         0.24789929 0.24789929 0.24789929 0.         0.\n",
      "  0.         0.         0.         0.24789929 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
